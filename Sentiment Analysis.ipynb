{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reviews_df= pd.read_csv('Amazon_Reviews.csv')\n",
    "\n",
    "reviews_df['Label']=reviews_df['Label'].map({'__label__2 ':1,'__label__1 ':0})\n",
    "\n",
    "y=reviews_df['Label']\n",
    "\n",
    "reviews_df.drop(columns='Label',axis=1,inplace=True)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(reviews_df,y,random_state=42,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuragsriram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "183    handful track hear far complete though missing...\n",
       "38     work mac clearly say line work mac o system di...\n",
       "24     liked album thought would heard song two thoug...\n",
       "142    pattern detailed sketch although excited purch...\n",
       "141    contemporary fairytale sure delight book take ...\n",
       "29     based review bought one glad vcr dvd early chr...\n",
       "19     size recomended size chart real size much smal...\n",
       "169    awesume best block ever toy grandson favorite ...\n",
       "127    one kind book timber frame building husband lo...\n",
       "118    got fooled know audio cd game thought could us...\n",
       "56     label memphis tn come reatards fronted course ...\n",
       "5      absolute masterpiece quite sure actually takin...\n",
       "122    good gave 5 star little dog much fun riping ap...\n",
       "113    hour fun 4 6 year old purchased game year ago ...\n",
       "93     slow dull classical 1980 style movie 2 star 5 ...\n",
       "139    charisma henry dog daughter 6 gave henry love ...\n",
       "95     worth time book wriiten horribly would never l...\n",
       "171    great value set incorporates lot favorite set ...\n",
       "196    guitar absentia due respect ambient music enth...\n",
       "168    great block daughter love block get look one f...\n",
       "140    found henry dog two henry dog one lost ride wa...\n",
       "31     happy jvc nut 3 jvc television one vcr one boo...\n",
       "12     great read thought book brilliant yet realisti...\n",
       "35     unfortunately entertaining least bit first ord...\n",
       "115    ddr cooool cd really great game long versons t...\n",
       "42     alaska sourdough read book visiting brother al...\n",
       "73     use imagination 3 year old daughter love softw...\n",
       "136    dont run get outrun unless 1985 outrun biggest...\n",
       "135    great graphic 2006 coast coast pc great graphi...\n",
       "51     nice money first one bought defective dvd work...\n",
       "                             ...                        \n",
       "48     terrible buy bought wife birthday toreturn dvd...\n",
       "88     buyer beware ordered cake topper june 27 2010 ...\n",
       "21     delicious cookie mix thought funny bought prod...\n",
       "57     either 1 5 star depends look either 1 5 star d...\n",
       "160    uh oh using brand x embarassment steer clear o...\n",
       "191    museum exhibit inside book book library geneal...\n",
       "129    hanford mill museum friend master carpenter sa...\n",
       "37     great playing larry work muse label late 80 ea...\n",
       "157    rest story knew something missing first book r...\n",
       "187    mask maker liked film every horror like type m...\n",
       "1      best soundtrack ever anything reading lot revi...\n",
       "52     disappointed romanian book opinion biased take...\n",
       "149    good song bad cover cd compilation set purchas...\n",
       "130    able buils timber frame house book vaguely bre...\n",
       "151    fooled bought set last november decided listen...\n",
       "103    best series te lindsay content sole occupant r...\n",
       "99     caution track original version recorded versio...\n",
       "116    amazing ordered cd take abit get live england ...\n",
       "87     comfy warm run big like title say warm comfy r...\n",
       "74     good ok rental game pretty exciting nice reall...\n",
       "121    letter home like letter home cuz tell story re...\n",
       "198    review pillow joke sending pillow back come cl...\n",
       "20     men ultrasheer model may ok sedentary type act...\n",
       "188    worst american b movie movie horror film thhat...\n",
       "71     barbie rapunzel cry child 6 year old daughter ...\n",
       "106    authentic first encounter yoruba say cd really...\n",
       "14     awful beyond belief feel write keep others was...\n",
       "92     omg soulwax owns wow like amazing album ever e...\n",
       "179    yet another unsubstantiated case believe discr...\n",
       "102    yes got book expecting much man wrong loved bo...\n",
       "Name: Cleaned_text, Length: 159, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocessing(review):\n",
    "    \n",
    "    final_tokens=' '\n",
    "    tokens=tokenizer.tokenize(review)\n",
    "    #print('Tokens:',tokens)\n",
    "    pure_tokens=[token.lower() for token in tokens if token.lower() not in stopwords.words('english')]\n",
    "    #print('Pure Tokens:',pure_tokens)\n",
    "    lemmas_tokens=[lemmatizer.lemmatize(pure_token) for pure_token in pure_tokens]\n",
    "    \n",
    "    final_tokens=final_tokens.join(lemmas_tokens)\n",
    "    \n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "#preprocessing('I was eating my breakfast when you were playing')\n",
    "X_train['Cleaned_text']=X_train['Review'].apply(preprocessing)\n",
    "X_train['Cleaned_text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words='english',use_idf=True)\n",
    "\n",
    "vectorizer.fit(X_train['Cleaned_text'])\n",
    "X_train_TfIdf=vectorizer.transform(X_train['Cleaned_text'])\n",
    "\n",
    "#vectorizer.get_feature_names()\n",
    "\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuragsriram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8533333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_TfIdf.toarray(),y_train)\n",
    "\n",
    "#clf.score(X_train_TfIdf.toarray(),y_train)\n",
    "\n",
    "\n",
    "#test_review= 'That product was its absolute best and I loved using every feature of it'\n",
    "X_test['Cleaned_text']=X_test['Review'].apply(preprocessing)\n",
    "\n",
    "#processed_test_review\n",
    "X_test_tfIdf=vectorizer.transform(X_test['Cleaned_text'])\n",
    "\n",
    "y_pred=clf.predict(X_test_tfIdf.toarray())\n",
    "\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n",
    "y_proba_pred=clf.predict_proba(X_test_tfIdf.toarray())[::,1]\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(y_test,y_proba_pred)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "roc_auc_score(y_test,y_proba_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuragsriram\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8533333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "logit=LogisticRegression()\n",
    "logit.fit(X_train_TfIdf,y_train)\n",
    "y_logistic_pred=logit.predict(X_test_tfIdf)\n",
    "\n",
    "y_logistic_proba_pred=logit.predict_proba(X_test_tfIdf.toarray())[::,1]\n",
    "\n",
    "log_fpr,log_tpr,log_thresholds=roc_curve(y_test,y_logistic_proba_pred)\n",
    "\n",
    "plt.plot(log_fpr,log_tpr)\n",
    "\n",
    "roc_auc_score(y_test,y_logistic_proba_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
